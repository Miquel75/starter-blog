---
title: 'Régression linéaire'
subtitle: 'Lier deux variables entre elles.'
summary: Une regression linéaire, à quoi ça sert ? 
authors:
- Miquel Pons

tags:
- R
- bases
- régression linéaire
- corrélation
- lm
- cor
- shapiro
- bptest
- lmtest

categories:
- R
- Statistiques
- Premiers pas
- Modélisation


date: Sys.Date()
lastmod: "2020-09-27T00:00:00Z"
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Placement options: 1 = Full column width, 2 = Out-set, 3 = Screen-width
# Focal point options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
image:
  placement: 1
#  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/CpkOjOcXdUY)'
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []


---



<div id="quest-ce-quune-régression-linéaire" class="section level2">
<h2>Qu’est ce qu’une régression linéaire ?</h2>
<p>C’est une modélisation statistique (une équation) qui cherche à relier de façon linéaire une variable dite expliquée, à une ou plusieurs autres variables, dites explicatives.</p>
<p>La régression linéaire simple avec une seule variable explicative c’est :<br />
<code>y = a.x + b + e</code> <br/>
y est la variable expliquée (dépendante), que nous souhaitons décrire et prédire. Souvent représentée sur l’axe des ordonnées. <br/>
x est la variable explicative (indépendante), permettant de décrire et prédire y, souvent représentée sur l’axe des abscisses. <br/>
b est l’ordonnée à l’origine. <br/>
e est l’erreur</p>
</div>
<div id="a-quoi-ça-sert" class="section level2">
<h2>A quoi ça sert ?</h2>
<p>Comprendre la relation entre la variable expliquée et la (ou les) variables explicatives.<br />
Prédire à partir de la variable explicative (prédictive dans ce cas là) quelle sera la valeur possible de la variable expliquée.</p>
</div>
<div id="en-pratique-ça-donne-quoi" class="section level2">
<h2>En pratique ça donne quoi ?</h2>
<p>Il ne faut pas oublier la question que l’on se pose : Dans une regression l’hypothèse nulle H0 est que la variable explicative ne permet pas d’expliquer la variable explicative.</p>
<p>Importer les données (à télécharger ici <a href="/files/donnees.csv">donnees.csv</a> ) et les manipuler pour avoir le tableau qui nous intéresse.</p>
<pre class="r"><code>library(dplyr)
library(tidyr)</code></pre>
<pre class="r"><code>#on importe les données
donnees&lt;-read.csv2(&#39;donnees.csv&#39;,header=TRUE,sep=&#39;;&#39;)

#Selectionner la variable expliquée et la (ou les) variables explicatives
names(donnees)</code></pre>
<pre><code>##  [1] &quot;CLEOBS&quot;                       &quot;OT&quot;                          
##  [3] &quot;TAILLE&quot;                       &quot;PROF&quot;                        
##  [5] &quot;ESP&quot;                          &quot;DATEDEBUT&quot;                   
##  [7] &quot;DATEFIN&quot;                      &quot;DATEDETERMINATIONOBS&quot;        
##  [9] &quot;X&quot;                            &quot;Y&quot;                           
## [11] &quot;PRECISION&quot;                    &quot;ORGANISMEOBSERVATEUR&quot;        
## [13] &quot;ORGANISMEGESTIONNAIREDONNEES&quot;</code></pre>
<pre class="r"><code>donnees_filt&lt;-donnees %&gt;%
 dplyr::select(TAILLE, PROF) 

#supprimer les lignes avec valeurs manquantes, fonction drop_na du package &#39;tidyr&#39;
donnees_na &lt;- donnees_filt %&gt;% drop_na(TAILLE, PROF)</code></pre>
<p>On regarde un peu ce qu’on a.</p>
<pre class="r"><code>summary(donnees_na)</code></pre>
<pre><code>##     TAILLE              PROF          
##  Length:26          Length:26         
##  Class :character   Class :character  
##  Mode  :character   Mode  :character</code></pre>
<p>La profondeur en class character peut être source de problème. On le veut en numérique.</p>
<pre class="r"><code>donnees_na$PROF&lt;-as.numeric(donnees_na$PROF)
donnees_na$TAILLE&lt;-as.numeric(donnees_na$TAILLE)

class(donnees_na$PROF)</code></pre>
<pre><code>## [1] &quot;numeric&quot;</code></pre>
<p>On produit notre modèle de régression linéaire entre la profondeur et la taille des poissons de notre échantillon et on visualise le résultat.</p>
<pre class="r"><code>#régression linéaire avec fonction lm (?lm pour l&#39;aide R)
mod&lt;-lm(TAILLE~PROF, donnees_na)
#visualiser les données
plot(donnees_na$PROF, donnees_na$TAILLE)
abline(mod)
title(&quot;Régression linéaire&quot;)
summary(mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = TAILLE ~ PROF, data = donnees_na)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.045  -3.511  -1.040   4.415  11.135 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -2.0334     2.5199  -0.807    0.428    
## PROF          1.0599     0.1272   8.331 1.53e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.761 on 24 degrees of freedom
## Multiple R-squared:  0.743,  Adjusted R-squared:  0.7323 
## F-statistic:  69.4 on 1 and 24 DF,  p-value: 1.53e-08</code></pre>
<pre class="r"><code>#visualiser les résidus, étant la distance des points par rapport au modèle
segments(donnees_na$PROF,fitted(mod),donnees_na$PROF, donnees_na$TAILLE)
#frame valeurs taille
min(donnees_na$PROF)</code></pre>
<pre><code>## [1] 5</code></pre>
<pre class="r"><code>max(donnees_na$PROF)</code></pre>
<pre><code>## [1] 40</code></pre>
<pre class="r"><code>pred.frame&lt;-data.frame(PROF= 5:40)
#predict permet des predictions d&#39;après les résultats du modèle
pc&lt;-predict(mod, interval=&quot;confidence&quot;, newdata=pred.frame)
pp&lt;-predict(mod, interval=&quot;prediction&quot;, newdata=pred.frame)
#valeur des intervalles de confiance haut et bas [,2:3]. lty le type de ligne tracés
matlines(pred.frame, pc[,2:3], lty=c(2,2), col=&quot;blue&quot;)
matlines(pred.frame, pp[,2:3], lty=c(3,3), col=&quot;red&quot;)
#légende des matlines
legend(&quot;topleft&quot;,c(&quot;confiance&quot;,&quot;prediction&quot;),lty=c(2,3), col=c(&quot;blue&quot;,&quot;red&quot;))
title(&quot;Régression linéaire&quot;)</code></pre>
<p><img src="/post/regression-lineaire/index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Regardons les résultats de la régression de plus près.</p>
<pre class="r"><code>summary(mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = TAILLE ~ PROF, data = donnees_na)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.045  -3.511  -1.040   4.415  11.135 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -2.0334     2.5199  -0.807    0.428    
## PROF          1.0599     0.1272   8.331 1.53e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.761 on 24 degrees of freedom
## Multiple R-squared:  0.743,  Adjusted R-squared:  0.7323 
## F-statistic:  69.4 on 1 and 24 DF,  p-value: 1.53e-08</code></pre>
<p><strong>Call</strong> -&gt; indique la formule utilisée.<br />
<strong>Residuals</strong> -&gt; indique la différence entre la valeur observée et la valeur prédite par le modèle.<br />
<strong>Coefficient</strong> -&gt; donne la valeur de la pente :
La pente correspond à l’évolution de la taille en fonction de la profondeur. Ici, pour 1 mètre de profondeur supplémentaire, la taille des poissons augmentera de 1.068cm. Intercept est la valeur du reste.<br />
-&gt; Std. Error la précision de l’estimation.<br />
-&gt; t value le coefficient (pente) / par Std. Error, test si le coeff est significativement différent de zéro.<br />
-&gt; Pr(&gt;|t|) le niveau de signification<br />
<strong>Residual Standard Error</strong> -&gt; déviation standard résiduel, c’est mieux quand ce chiffre est petit.<br />
<strong>Multiple R-squared</strong> -&gt; R² est la proportion de variance expliquée par le modèle. <em>Plus R² est proche de 1, plus la variabilité des données est expliqué par le modèle.</em> Plus les résidus sont petits, plus R² est grand. Adjusted R-squared est plus précis dans le cas d’une régression multiple. La racine carré de R² donne le coefficient de corrélation.<br />
<strong>F-statistic</strong> -&gt; vérifie si le poids d’une variable est significativement différent de zéro. Si la p-value est suppérieur à 0.05, le modèle ne fait rien d’intéressant. Ici ce n’est pas le cas, la variable explicative de la profondeur semble affecter la taille des poissons.</p>
<p>Le calcul du <strong>coefficient de corrélation</strong> peut aussi se faire avec la fonction <code>cor</code></p>
<pre class="r"><code>cor(donnees_na$PROF, donnees_na$TAILLE)</code></pre>
<pre><code>## [1] 0.8619939</code></pre>
<p>On voit que le coefficient de corrélation entre la profondeur et la taille des poissons de notre échantillon est de 0.7866876.</p>
<p>Si R = 0, il n’y a pas de relation linéaire entre les variables, si R = 1 la relation linéaire est parfaite, tous les points sont sur la droite.</p>
<p>Les variables PROF et TAILLE semblent reliées étant donné la correlation entre ces variables, le coefficient de la pente et la p-value. Etant donné le R² la profondeur a un fort impact sur la taille mais il reste cependant une partie inexpliquée.</p>
</div>
<div id="valider-le-modèle-en-testant-les-hypothèses-sur-lequel-il-repose" class="section level2">
<h2>Valider le modèle en testant les hypothèses sur lequel il repose</h2>
<p>La régression linéaire repose sur des hypothèses: normalité, linearité, homoscedasticité, and absence of multicollinearity</p>
<p><strong>Normalité</strong></p>
<p>Afin de faire des inférences valides issu de ce modèle (pour vérifier les prédictions basées sur cette droite), il faut tester la normalité des erreurs résiduels. Les valeurs résiduels doivent suivre une distribution normale pour que le modèle soit robuste.</p>
<p>Plusieurs manière de tester cela:</p>
<p>Un histograme pour visualiser si la distribution suit une loi normale</p>
<pre class="r"><code>hist(mod$residuals)</code></pre>
<p><img src="/post/regression-lineaire/index_files/figure-html/unnamed-chunk-8-1.png" width="672" />
Un qqplot(quantile-quantile) comparant la distribution des résidus à une distribution normale</p>
<pre class="r"><code>qqnorm(mod$residuals)
qqline(mod$residuals)</code></pre>
<p><img src="/post/regression-lineaire/index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Ces deux graphiques semblent indiqués une loi normale.</p>
<p>Un shapiro test dont l’hypothèse H0 est que l’échantillon suit une loi normale</p>
<pre class="r"><code>shapiro.test(mod$residuals)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  mod$residuals
## W = 0.97797, p-value = 0.8282</code></pre>
<p>La p-value est supérieur à 0.05, on ne peut PAS dire que les résidus ne suivent PAS une loi normale.</p>
<p>L’erreur résiduel semble bien suivre une loi normale.</p>
<p><strong>L’homoscédasticité</strong></p>
<p>C’est quand la dispersion est homogène, l’ensemble des erreurs à une variance similaire.</p>
<pre class="r"><code>par(mfrow=c(2,2)) #rentrer 4 plots 2x2
plot(mod)</code></pre>
<p><img src="/post/regression-lineaire/index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Plus la ligne rouge est proche de zéro plus il y a de l’homoscédasticité. Plus elle s’en sépare plus il y a de l’hétéroscédasticité.
On peut le tester statistiquement avec le test de Breusch Pagan et le ncvTest.</p>
<pre class="r"><code>library(lmtest)
library(car)</code></pre>
<pre class="r"><code>lmtest::bptest(mod)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  mod
## BP = 2.4749, df = 1, p-value = 0.1157</code></pre>
<pre class="r"><code>car::ncvTest(mod)</code></pre>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 1.396593, Df = 1, p = 0.23729</code></pre>
<p>La P-value est supérieur à 0.05. On ne peut pas rejeter l’hypothèse nulle, de l’homoscédasticité. des données. Ces données auront donc besoin d’être plus travaillées (transformation log ou autre) pour obtenir un modèle robuste.</p>
<p><strong>Linéarité</strong></p>
<p>Les variables ont une relation linéaire. Lorsque la normalité et l’homoscédasticité sont attestés, il n’y a pas a s’inquiété pour ça.</p>
<p><strong>Multicollinearité</strong></p>
<p>C’est lorsque des variables explicatives sont très corrélés les unes aux autres, cela peut fausser les résultats. Il faut donc vérifier les coefficients de corrélation, voir si certaines dépassent 0.80, et vérifier le VIF. Nul besoin de s’en inquiéter ici nous n’avons qu’une seule variable explicative (indépendante).</p>
</div>
